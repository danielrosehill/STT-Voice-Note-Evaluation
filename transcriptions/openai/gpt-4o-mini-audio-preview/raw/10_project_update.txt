Quick update on the voice notes evaluation project: I've been collecting transcripts from my actual usage over the past few weeks, and I think I've enough data now to start doing some meaningful comparisons. The dataset is pretty diverse, everything from technical notes about Docker and Git workflows to parenting questions about Dove. What's interesting is seeing how different STT services handle the mix of technical jargon and everyday speech. Whisper seems to do really well with the technical terms, probably because it's trained on a lot of code and documentation, but sometimes it struggles with the more conversational, stream-of-consciousness style that I use when I'm just thinking out loud. Deepgram is faster but occasionally misses context clues that help with ambiguous words. I'm thinking the next step is to create some synthetic data to fill in gaps where I don't have enough examples, like I don't have many voice notes about specific topics that would be good test cases. Maybe I should record some intentionally to round out the dataset. Oh, and I should probably anonymize some of the personal stuff before I use it for evaluation. Don't want to accidentally include private information about Dove or Sarah in a public dataset.