So I just finished a call with the team about the new AI automation project. Let me capture the key points while they're still fresh. Sarah brought up a good point about the cost implications of using GPT four for everything. She's right. We're burning through API credits pretty quickly. Mike suggested we look into fine tuning a smaller model, maybe llama two or something open source that we can run locally. The challenges we need to set up the infrastructure for that. Probably some GPU instances on AWS, or maybe just beef up our current server. Jennifer from product was asking about timelines. I told her we could probably have a proof of concept ready in two weeks if we stick with the API approach, but if we go with the self-hosted route, it might take a month or more. We also talked about the Docker setup. The current images are getting pretty bloated, like three gigs each, which is slowing down deployment. I think we need to do some cleanup, maybe use multi-stage builds or switch to alpine based images. Action items. I'm going to research the cost of different approaches and put together a comparison. Sarah will look into the infrastructure requirements for self-hosting. Mike will start experimenting with model fine tuning. We'll reconvene next Friday to make a decision.